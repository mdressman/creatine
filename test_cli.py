"""CLI for managing datasets and running security tests.

NOTE: This is the legacy CLI. For new projects, use `python creatine_cli.py` instead.
"""

import asyncio
import argparse
import os
from pathlib import Path

from dotenv import load_dotenv

from testing import (
    Dataset, DatasetRegistry, TestPrompt, AttackType, Severity,
    load_from_csv, load_from_huggingface,
    TestHarness, print_progress
)
from creatine import ThreatDetector, PromptIntelFeedClient
from creatine.detector import FEED_RULES_PATH

# Backward compatibility aliases
PromptIntelClient = ThreatDetector

load_dotenv()


def cmd_list(args, registry: DatasetRegistry):
    """List available datasets."""
    datasets = registry.list_datasets()
    if not datasets:
        print("No datasets found. Add datasets to the 'datasets/' directory.")
        return
    
    print("Available datasets:")
    for name in datasets:
        dataset = registry.get(name)
        if dataset:
            malicious = len([p for p in dataset.prompts if p.is_malicious])
            benign = len(dataset) - malicious
            print(f"  â€¢ {name}: {len(dataset)} prompts ({malicious} malicious, {benign} benign)")


def cmd_info(args, registry: DatasetRegistry):
    """Show detailed info about a dataset."""
    dataset = registry.get(args.name)
    if not dataset:
        print(f"Dataset not found: {args.name}")
        return
    
    print(f"Dataset: {dataset.name}")
    print(f"Description: {dataset.description}")
    print(f"Version: {dataset.version}")
    print(f"Source: {dataset.source_url or 'N/A'}")
    print(f"Total prompts: {len(dataset)}")
    print()
    
    # Attack type breakdown
    print("Attack types:")
    for attack_type in AttackType:
        count = len(dataset.filter_by_type(attack_type))
        if count > 0:
            print(f"  {attack_type.value}: {count}")
    
    # Severity breakdown
    print("\nSeverity levels:")
    for severity in Severity:
        count = len(dataset.filter_by_severity(severity))
        if count > 0:
            print(f"  {severity.value}: {count}")


def cmd_sample(args, registry: DatasetRegistry):
    """Show sample prompts from a dataset."""
    dataset = registry.get(args.name)
    if not dataset:
        print(f"Dataset not found: {args.name}")
        return
    
    samples = dataset.sample(args.count)
    for i, prompt in enumerate(samples, 1):
        status = "ðŸ”´ Malicious" if prompt.is_malicious else "ðŸŸ¢ Benign"
        print(f"\n[{i}] {status} ({prompt.attack_type.value}, {prompt.severity.value})")
        print(f"    {prompt.prompt[:200]}{'...' if len(prompt.prompt) > 200 else ''}")


def cmd_import_hf(args, registry: DatasetRegistry):
    """Import a dataset from HuggingFace."""
    print(f"Importing from HuggingFace: {args.dataset}")
    try:
        dataset = load_from_huggingface(
            args.dataset,
            split=args.split,
            prompt_field=args.prompt_field,
            label_field=args.label_field,
        )
        registry.save_dataset(dataset)
        print(f"Imported {len(dataset)} prompts as '{dataset.name}'")
    except Exception as e:
        print(f"Error importing: {e}")


def cmd_import_csv(args, registry: DatasetRegistry):
    """Import a dataset from CSV."""
    path = Path(args.file)
    if not path.exists():
        print(f"File not found: {path}")
        return
    
    dataset = load_from_csv(path, args.prompt_col, args.label_col)
    registry.save_dataset(dataset)
    print(f"Imported {len(dataset)} prompts as '{dataset.name}'")


async def cmd_test(args, registry: DatasetRegistry):
    """Run tests on a dataset."""
    api_key = os.getenv("PROMPTINTEL_API_KEY")
    if not api_key:
        print("Warning: PROMPTINTEL_API_KEY not set. API calls may fail.")
    
    # Determine which rule sets to use
    include_feed = not args.default_only
    
    if args.compare:
        # Run comparison mode - test with both rule sets
        await run_comparison_test(args, registry, api_key)
        return
    
    # Check for advanced evaluation modes
    # --enable-llm implies --enable-semantics for full evaluation
    enable_llm = getattr(args, 'enable_llm', False)
    enable_semantics = getattr(args, 'enable_semantics', False) or enable_llm
    
    client = PromptIntelClient(
        api_key or "", 
        verbose=args.verbose,
        include_feed_rules=include_feed,
        enable_llm=enable_llm,
        enable_semantics=enable_semantics,
    )
    
    rules_desc = "default + feed" if include_feed else "default only"
    if enable_llm:
        rules_desc += " + LLM + semantic"
    elif enable_semantics:
        rules_desc += " + semantic"
    print(f"Using rules: {rules_desc} ({client.rules_info})")
    
    harness = TestHarness(client, registry)
    
    try:
        if args.name == "all":
            reports = await harness.run_all(
                concurrency=args.concurrency,
                progress_callback=print_progress if not args.quiet and not args.verbose else None,
                verbose=args.verbose,
            )
            print("\n")
            for name, report in reports.items():
                print(report.summary())
                if args.save:
                    path = harness.save_report(report)
                    print(f"Saved to: {path}")
        else:
            dataset = registry.get(args.name)
            if not dataset:
                print(f"Dataset not found: {args.name}")
                return
            
            print(f"Testing {len(dataset)} prompts from '{args.name}'...")
            report = await harness.run_dataset(
                dataset,
                concurrency=args.concurrency,
                progress_callback=print_progress if not args.quiet and not args.verbose else None,
                verbose=args.verbose,
            )
            print("\n" + report.summary())
            
            if args.save:
                suffix = "_default_only" if args.default_only else ""
                path = harness.save_report(report, suffix=suffix)
                print(f"Report saved to: {path}")
    finally:
        await client.close()


async def run_comparison_test(args, registry: DatasetRegistry, api_key: str):
    """Run tests with all evaluation modes and compare results."""
    dataset = registry.get(args.name)
    if not dataset:
        print(f"Dataset not found: {args.name}")
        return
    
    print(f"=== Evaluation Mode Comparison: {args.name} ({len(dataset)} prompts) ===\n")
    
    reports = {}
    
    # Mode 1: Keywords only (fastest)
    print("[1/4] Running with KEYWORDS only...")
    client = PromptIntelClient(api_key or "", verbose=False, include_feed_rules=True)
    harness = TestHarness(client, registry)
    reports['keywords'] = await harness.run_dataset(
        dataset, concurrency=args.concurrency,
        progress_callback=print_progress if not args.quiet else None,
    )
    await client.close()
    
    # Mode 2: Keywords + Semantics
    print("\n\n[2/4] Running with KEYWORDS + SEMANTICS...")
    client = PromptIntelClient(api_key or "", verbose=False, include_feed_rules=True, enable_semantics=True)
    harness = TestHarness(client, registry)
    reports['semantics'] = await harness.run_dataset(
        dataset, concurrency=args.concurrency,
        progress_callback=print_progress if not args.quiet else None,
    )
    await client.close()
    
    # Mode 3: Keywords + Semantics + LLM (most accurate)
    print("\n\n[3/4] Running with KEYWORDS + SEMANTICS + LLM...")
    client = PromptIntelClient(api_key or "", verbose=False, include_feed_rules=True, enable_semantics=True, enable_llm=True)
    harness = TestHarness(client, registry)
    reports['llm'] = await harness.run_dataset(
        dataset, concurrency=args.concurrency,
        progress_callback=print_progress if not args.quiet else None,
    )
    await client.close()
    
    # Mode 4: Default rules only (baseline)
    print("\n\n[4/4] Running with DEFAULT rules only (baseline)...")
    client = PromptIntelClient(api_key or "", verbose=False, include_feed_rules=False)
    harness = TestHarness(client, registry)
    reports['baseline'] = await harness.run_dataset(
        dataset, concurrency=args.concurrency,
        progress_callback=print_progress if not args.quiet else None,
    )
    await client.close()
    
    # Print comparison table
    print("\n\n")
    print("=" * 95)
    print("                              EVALUATION MODE COMPARISON")
    print("=" * 95)
    print(f"{'Metric':<20} {'Baseline':>14} {'Keywords':>14} {'+ Semantics':>14} {'+ LLM':>14} {'Improvement':>12}")
    print("-" * 95)
    
    r = reports
    metrics = [
        ("Accuracy", 'accuracy'),
        ("Precision", 'precision'),
        ("Recall", 'recall'),
        ("F1 Score", 'f1_score'),
    ]
    
    for name, attr in metrics:
        baseline = getattr(r['baseline'], attr)
        kw = getattr(r['keywords'], attr)
        sem = getattr(r['semantics'], attr)
        llm = getattr(r['llm'], attr)
        improvement = llm - baseline
        arrow = "â†‘" if improvement > 0 else ("â†“" if improvement < 0 else "â†’")
        print(f"{name:<20} {baseline:>13.2%} {kw:>13.2%} {sem:>13.2%} {llm:>13.2%} {arrow} {improvement:>+10.2%}")
    
    print("-" * 95)
    print(f"{'True Positives':<20} {r['baseline'].true_positives:>14} {r['keywords'].true_positives:>14} {r['semantics'].true_positives:>14} {r['llm'].true_positives:>14} {r['llm'].true_positives - r['baseline'].true_positives:>+12}")
    print(f"{'True Negatives':<20} {r['baseline'].true_negatives:>14} {r['keywords'].true_negatives:>14} {r['semantics'].true_negatives:>14} {r['llm'].true_negatives:>14} {r['llm'].true_negatives - r['baseline'].true_negatives:>+12}")
    print(f"{'False Positives':<20} {r['baseline'].false_positives:>14} {r['keywords'].false_positives:>14} {r['semantics'].false_positives:>14} {r['llm'].false_positives:>14} {r['llm'].false_positives - r['baseline'].false_positives:>+12}")
    print(f"{'False Negatives':<20} {r['baseline'].false_negatives:>14} {r['keywords'].false_negatives:>14} {r['semantics'].false_negatives:>14} {r['llm'].false_negatives:>14} {r['llm'].false_negatives - r['baseline'].false_negatives:>+12}")
    print("-" * 95)
    print(f"{'Avg Time (ms)':<20} {r['baseline'].avg_response_time_ms:>14.1f} {r['keywords'].avg_response_time_ms:>14.1f} {r['semantics'].avg_response_time_ms:>14.1f} {r['llm'].avg_response_time_ms:>14.1f}")
    print("=" * 95)
    
    # Summary
    print("\nSummary:")
    print(f"  â€¢ Baseline (default rules only): {r['baseline'].f1_score:.1%} F1, {r['baseline'].avg_response_time_ms:.1f}ms")
    print(f"  â€¢ Keywords (+ feed rules):       {r['keywords'].f1_score:.1%} F1, {r['keywords'].avg_response_time_ms:.1f}ms")
    print(f"  â€¢ + Semantics:                   {r['semantics'].f1_score:.1%} F1, {r['semantics'].avg_response_time_ms:.1f}ms")
    print(f"  â€¢ + LLM (full):                  {r['llm'].f1_score:.1%} F1, {r['llm'].avg_response_time_ms:.1f}ms")
    
    # Save reports if requested
    if args.save:
        for mode, report in reports.items():
            path = harness.save_report(report, suffix=f"_{mode}")
            print(f"  Saved: {path}")


def cmd_add(args, registry: DatasetRegistry):
    """Add a new prompt to a dataset."""
    dataset = registry.get(args.dataset)
    if not dataset:
        # Create new dataset
        dataset = Dataset(
            name=args.dataset,
            description=f"Custom dataset: {args.dataset}",
            prompts=[],
        )
    
    prompt = TestPrompt(
        prompt=args.prompt,
        is_malicious=not args.benign,
        attack_type=AttackType(args.type) if args.type else AttackType.UNKNOWN,
        severity=Severity(args.severity) if args.severity else Severity.MEDIUM,
        description=args.description or "",
        tags=args.tags.split(",") if args.tags else [],
    )
    
    dataset.prompts.append(prompt)
    registry.save_dataset(dataset)
    print(f"Added prompt to '{args.dataset}' (total: {len(dataset)})")


def cmd_sync_feed(args):
    """Sync IoPC feed from PromptIntel and generate Nova rules."""
    api_key = args.api_key or os.getenv("PROMPTINTEL_API_KEY")
    if not api_key:
        print("Error: PROMPTINTEL_API_KEY not set. Provide via --api-key or environment variable.")
        print("Get your API key from: https://promptintel.novahunting.ai/")
        return
    
    output_path = Path(args.output) if args.output else FEED_RULES_PATH
    
    try:
        if args.smart:
            # Use Rule Generation Agent for AI-powered rule generation
            from rule_agent import RuleGenerationAgent
            
            print("Using AI-powered rule generation (requires Azure OpenAI)...")
            agent = RuleGenerationAgent(verbose=args.verbose)
            agent.add_promptintel_source()
            
            # Run without optimization (just generate from feed)
            result = asyncio.run(agent.run(
                test_dataset=None,  # No test dataset = no optimization
                output_file=output_path.name,
                max_iterations=1,
            ))
            
            if result and result.output_file:
                result_path = Path("rules") / result.output_file
                print(f"\nâœ“ Feed rules synced to: {result_path}")
                print(f"  Rules will be loaded automatically on next test run.")
                if args.verbose:
                    content = result_path.read_text()
                    rules_count = content.count("rule ")
                    print(f"  Generated {rules_count} rules from feed")
            else:
                print("Error: Rule generation failed")
        else:
            # Use simple keyword extraction (no AI)
            from rule_agent import generate_simple_rules
            
            if args.verbose:
                print("Fetching IoPC feed from PromptIntel...")
            
            client = PromptIntelFeedClient(api_key, verbose=args.verbose)
            try:
                indicators = client.fetch_all()
                
                if args.verbose:
                    print(f"Retrieved {len(indicators)} indicators")
                
                if not indicators:
                    raise ValueError("No indicators retrieved from feed")
                
                rules = generate_simple_rules(indicators)
                
                # Ensure directory exists
                output_path.parent.mkdir(parents=True, exist_ok=True)
                output_path.write_text(rules)
                
                print(f"\nâœ“ Feed rules synced to: {output_path}")
                print(f"  Rules will be loaded automatically on next test run.")
                
                if args.verbose:
                    rules_count = rules.count("rule ")
                    print(f"  Generated {rules_count} rules from feed")
            finally:
                client.close()
        
    except Exception as e:
        print(f"Error syncing feed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


def cmd_feed_preview(args):
    """Preview IoPC indicators from the feed without generating rules."""
    api_key = args.api_key or os.getenv("PROMPTINTEL_API_KEY")
    if not api_key:
        print("Error: PROMPTINTEL_API_KEY not set.")
        return
    
    client = PromptIntelFeedClient(api_key, verbose=args.verbose)
    try:
        indicators, total = client.fetch_prompts(
            page=1,
            limit=args.limit,
            severity=args.severity,
            category=args.category,
        )
        
        print(f"\n=== PromptIntel Feed Preview ({len(indicators)} of {total} total) ===\n")
        
        for iopc in indicators:
            severity_icon = {"critical": "ðŸ”´", "high": "ðŸŸ ", "medium": "ðŸŸ¡", "low": "ðŸŸ¢"}.get(
                iopc.risk_score.lower(), "âšª"
            )
            print(f"{severity_icon} [{iopc.risk_score.upper()}] {iopc.id}")
            print(f"   Category: {iopc.category}")
            if iopc.pattern:
                print(f"   Pattern: {iopc.pattern[:80]}{'...' if len(iopc.pattern) > 80 else ''}")
            if iopc.description:
                print(f"   Desc: {iopc.description[:80]}{'...' if len(iopc.description) > 80 else ''}")
            print()
            
    except Exception as e:
        print(f"Error fetching feed: {e}")
    finally:
        client.close()


async def cmd_generate_rules(args):
    """Run the Rule Generation Agent to create optimized Nova rules."""
    from rule_agent import RuleGenerationAgent
    
    try:
        agent = RuleGenerationAgent(verbose=args.verbose)
        
        # Add data sources
        if not args.no_promptintel:
            agent.add_promptintel_source()
        
        if args.add_huggingface:
            for dataset in args.add_huggingface:
                agent.add_huggingface_source(dataset)
        
        # Run optimization
        result = await agent.run(
            test_dataset=args.test_dataset,
            output_file=args.output,
            target_precision=args.target_precision,
            target_recall=args.target_recall,
            max_iterations=args.max_iterations,
        )
        
        print(f"\nâœ“ Rule generation complete!")
        print(f"  Output: rules/{args.output}")
        print(f"  Final F1: {result.final_metrics['f1']:.2%}")
        
    except Exception as e:
        print(f"Error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


async def cmd_adaptive(args, registry: DatasetRegistry):
    """Run adaptive detection on prompts or a dataset."""
    from creatine import AdaptiveDetector, AdaptiveConfig
    
    config = AdaptiveConfig(
        high_confidence_threshold=args.confidence_threshold,
        max_time_budget_ms=args.time_budget,
        force_full_analysis=args.force_full,
    )
    
    detector = AdaptiveDetector(config=config, verbose=args.verbose)
    
    if args.prompt:
        # Single prompt analysis
        result = await detector.analyze(args.prompt)
        
        print(f"\n{'='*60}")
        print(f"{'ðŸš¨ THREAT DETECTED' if result.is_threat else 'âœ… CLEAN'}")
        print(f"{'='*60}")
        print(f"Confidence: {result.confidence:.1%}")
        print(f"Risk Score: {result.risk_score}")
        print(f"Tier Used: {result.tier_used.name}")
        print(f"Total Time: {result.total_time_ms:.1f}ms")
        print(f"Cost Saved: {result.cost_saved}")
        
        if result.attack_types:
            print(f"Attack Types: {', '.join(result.attack_types)}")
        
        if result.escalation_path:
            print(f"\nEscalation Path:")
            for tier, reason in result.escalation_path:
                print(f"  {tier.name} â†’ {reason.value}")
        
    elif args.dataset:
        # Dataset analysis
        dataset = registry.get(args.dataset)
        if not dataset:
            print(f"Dataset not found: {args.dataset}")
            return
        
        print(f"Adaptive Detection: {args.dataset} ({len(dataset)} prompts)")
        print(f"Config: threshold={args.confidence_threshold}, budget={args.time_budget}ms")
        print()
        
        # Run adaptive detection on all prompts
        correct = 0
        tier_counts = {1: 0, 2: 0, 3: 0}
        total_time = 0
        
        for i, prompt in enumerate(dataset.prompts):
            result = await detector.analyze(prompt.prompt)
            
            # Check if prediction matches label
            is_correct = (result.is_threat == prompt.is_malicious)
            if is_correct:
                correct += 1
            
            tier_counts[result.tier_used.value] += 1
            total_time += result.total_time_ms
            
            # Progress
            if not args.quiet and (i + 1) % 10 == 0:
                pct = (i + 1) / len(dataset) * 100
                print(f"  Progress: {i+1}/{len(dataset)} ({pct:.0f}%)", end="\r")
        
        print()
        
        # Results
        stats = detector.get_stats()
        accuracy = correct / len(dataset)
        
        print(f"\n{'='*60}")
        print(f"ADAPTIVE DETECTION RESULTS")
        print(f"{'='*60}")
        print(f"Accuracy: {accuracy:.1%} ({correct}/{len(dataset)})")
        print(f"Total Time: {total_time:.1f}ms")
        print(f"Avg Time: {stats['avg_time_ms']:.1f}ms per prompt")
        print()
        print(f"Tier Distribution:")
        print(f"  Tier 1 (Keywords):  {tier_counts[1]:3d} ({stats['tier1_stop_rate']:.1%})")
        print(f"  Tier 2 (Semantics): {tier_counts[2]:3d} ({stats['tier2_stop_rate']:.1%})")
        print(f"  Tier 3 (LLM):       {tier_counts[3]:3d} ({stats['tier3_stop_rate']:.1%})")
        print(f"{'='*60}")
        
        # Compare to full LLM mode estimate
        full_llm_time_est = len(dataset) * 5000  # ~5s per prompt
        actual_time = total_time
        savings_pct = (1 - actual_time / full_llm_time_est) * 100
        print(f"\nEstimated savings vs full LLM mode: ~{savings_pct:.0f}%")
        print(f"  Full LLM estimate: ~{full_llm_time_est/1000:.0f}s")
        print(f"  Adaptive actual:   {actual_time/1000:.1f}s")
    else:
        print("Error: Provide --prompt or --dataset")


async def cmd_analyze_prompt(args):
    """Quick single-prompt analysis (user-friendly entry point)."""
    from creatine import AdaptiveDetector
    
    detector = AdaptiveDetector(verbose=args.verbose)
    result = await detector.analyze(args.prompt)
    
    # Simple output for quick use
    status = "ðŸš¨ THREAT" if result.is_threat else "âœ… SAFE"
    print(f"{status} | {result.risk_score} | {result.confidence:.0%} confidence | {result.tier_used.name} | {result.total_time_ms:.0f}ms")
    
    if result.attack_types:
        print(f"   Attack types: {', '.join(result.attack_types)}")


def main():
    parser = argparse.ArgumentParser(
        description="Security Agent Test Harness",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    subparsers = parser.add_subparsers(dest="command", help="Commands")
    
    # list command
    subparsers.add_parser("list", help="List available datasets")
    
    # info command
    info_parser = subparsers.add_parser("info", help="Show dataset details")
    info_parser.add_argument("name", help="Dataset name")
    
    # sample command
    sample_parser = subparsers.add_parser("sample", help="Show sample prompts")
    sample_parser.add_argument("name", help="Dataset name")
    sample_parser.add_argument("-n", "--count", type=int, default=5, help="Number of samples")
    
    # import-hf command
    hf_parser = subparsers.add_parser("import-hf", help="Import from HuggingFace")
    hf_parser.add_argument("dataset", help="HuggingFace dataset name (e.g., 'deepset/prompt-injections')")
    hf_parser.add_argument("--split", default="train", help="Dataset split")
    hf_parser.add_argument("--prompt-field", default="prompt", help="Field containing prompt text")
    hf_parser.add_argument("--label-field", default="label", help="Field containing label")
    
    # import-csv command
    csv_parser = subparsers.add_parser("import-csv", help="Import from CSV file")
    csv_parser.add_argument("file", help="Path to CSV file")
    csv_parser.add_argument("--prompt-col", default="prompt", help="Column with prompt text")
    csv_parser.add_argument("--label-col", default="label", help="Column with label")
    
    # test command
    test_parser = subparsers.add_parser("test", help="Run tests on a dataset")
    test_parser.add_argument("name", help="Dataset name (or 'all' for all datasets)")
    test_parser.add_argument("-c", "--concurrency", type=int, default=5, help="Concurrent requests")
    test_parser.add_argument("-q", "--quiet", action="store_true", help="Suppress progress output")
    test_parser.add_argument("-s", "--save", action="store_true", help="Save report to file")
    test_parser.add_argument("-v", "--verbose", action="store_true", help="Show detailed API responses")
    test_parser.add_argument("--default-only", action="store_true", help="Use only default rules (no feed rules)")
    test_parser.add_argument("--compare", action="store_true", help="Compare default vs default+feed rules")
    test_parser.add_argument("--enable-llm", action="store_true", help="Enable LLM-based rule evaluation (slower, more accurate)")
    test_parser.add_argument("--enable-semantics", action="store_true", help="Enable semantic similarity matching")
    
    # add command
    add_parser = subparsers.add_parser("add", help="Add a prompt to a dataset")
    add_parser.add_argument("dataset", help="Dataset name (created if doesn't exist)")
    add_parser.add_argument("prompt", help="The prompt text")
    add_parser.add_argument("--benign", action="store_true", help="Mark as benign (default: malicious)")
    add_parser.add_argument("--type", choices=[t.value for t in AttackType], help="Attack type")
    add_parser.add_argument("--severity", choices=[s.value for s in Severity], help="Severity level")
    add_parser.add_argument("--description", help="Description of the prompt")
    add_parser.add_argument("--tags", help="Comma-separated tags")
    
    # sync-feed command
    sync_parser = subparsers.add_parser("sync-feed", help="Sync IoPC feed and generate Nova rules")
    sync_parser.add_argument("--api-key", help="PromptIntel API key (or set PROMPTINTEL_API_KEY)")
    sync_parser.add_argument("-o", "--output", help="Output path for generated rules")
    sync_parser.add_argument("-v", "--verbose", action="store_true", help="Show detailed output")
    sync_parser.add_argument("--smart", action="store_true", help="Use AI to generate sophisticated rules (requires Azure OpenAI)")
    
    # feed-preview command
    preview_parser = subparsers.add_parser("feed-preview", help="Preview IoPC indicators from feed")
    preview_parser.add_argument("--api-key", help="PromptIntel API key (or set PROMPTINTEL_API_KEY)")
    preview_parser.add_argument("-n", "--limit", type=int, default=10, help="Number of indicators to show")
    preview_parser.add_argument("--severity", choices=["critical", "high", "medium", "low"], help="Filter by severity")
    preview_parser.add_argument("--category", help="Filter by category")
    preview_parser.add_argument("-v", "--verbose", action="store_true", help="Show HTTP requests")
    
    # generate-rules command (Rule Generation Agent)
    gen_parser = subparsers.add_parser("generate-rules", help="Run Rule Generation Agent to create optimized Nova rules")
    gen_parser.add_argument("--test-dataset", required=True, help="Dataset to test rules against")
    gen_parser.add_argument("--output", default="agent_optimized.nov", help="Output filename for rules")
    gen_parser.add_argument("--target-precision", type=float, default=0.90, help="Target precision (default: 0.90)")
    gen_parser.add_argument("--target-recall", type=float, default=0.80, help="Target recall (default: 0.80)")
    gen_parser.add_argument("--max-iterations", type=int, default=5, help="Max optimization iterations")
    gen_parser.add_argument("--no-promptintel", action="store_true", help="Don't use PromptIntel as data source")
    gen_parser.add_argument("--add-huggingface", action="append", metavar="DATASET", help="Add HuggingFace dataset as source")
    gen_parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
    
    # adaptive command (Adaptive Detection)
    adaptive_parser = subparsers.add_parser("adaptive", help="Run adaptive tiered detection (optimizes cost vs accuracy)")
    adaptive_parser.add_argument("--prompt", help="Analyze a single prompt")
    adaptive_parser.add_argument("--dataset", help="Analyze prompts from a dataset")
    adaptive_parser.add_argument("--confidence-threshold", type=float, default=0.85, help="Confidence threshold to stop escalation (default: 0.85)")
    adaptive_parser.add_argument("--time-budget", type=float, default=10000, help="Max time budget in ms (default: 10000)")
    adaptive_parser.add_argument("--force-full", action="store_true", help="Force full analysis through all tiers (for testing)")
    adaptive_parser.add_argument("-q", "--quiet", action="store_true", help="Suppress progress output")
    adaptive_parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output showing each tier's analysis")
    
    # analyze command (Quick single prompt)
    analyze_parser = subparsers.add_parser("analyze", help="Quick single-prompt security analysis")
    analyze_parser.add_argument("prompt", help="The prompt to analyze")
    analyze_parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    registry = DatasetRegistry(Path(__file__).parent / "datasets")
    
    if args.command == "list":
        cmd_list(args, registry)
    elif args.command == "info":
        cmd_info(args, registry)
    elif args.command == "sample":
        cmd_sample(args, registry)
    elif args.command == "import-hf":
        cmd_import_hf(args, registry)
    elif args.command == "import-csv":
        cmd_import_csv(args, registry)
    elif args.command == "test":
        asyncio.run(cmd_test(args, registry))
    elif args.command == "add":
        cmd_add(args, registry)
    elif args.command == "sync-feed":
        cmd_sync_feed(args)
    elif args.command == "feed-preview":
        cmd_feed_preview(args)
    elif args.command == "generate-rules":
        asyncio.run(cmd_generate_rules(args))
    elif args.command == "adaptive":
        asyncio.run(cmd_adaptive(args, registry))
    elif args.command == "analyze":
        asyncio.run(cmd_analyze_prompt(args))


if __name__ == "__main__":
    main()
